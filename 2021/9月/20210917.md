# Temporal ROI Align for Video Object Recognition(AAAI2021)
## Motivation: 解决视频目标检测中RoI Align操作缺少时序信息的问题，改善运动模糊、遮挡等情况下检测器的性能
## Method：提出了一个时序RoI Align模块，用来替代传统的RoI Align操作；具体的做法是在整个视频序列中均匀采样一些支持帧（Support Frame），在这些支持帧中寻找与当前帧RoI Feature最相似的RoI Feature，再通过时序注意力机制对当前帧+支持帧的RoI Feature进行聚合；具体的网络结构设计如下图所示：

<div align="center">
<img src="https://user-images.githubusercontent.com/43487243/133734447-c27447f6-d0c5-4044-9a62-e65477a38d4b.png" width = "550" height = "220">

时序RoI Align与传统RoI Align的区别（用到了支持帧的时序信息）

<img src="https://user-images.githubusercontent.com/43487243/133734464-ada74a9a-a4b4-4676-957a-76f6128203fb.png" width = "550" height = "270">
  
方法总览：（a）提取当前帧RoI特征（b）找到每个支持帧对应的最相似RoI特征（c）时序注意力模块

<img src="https://user-images.githubusercontent.com/43487243/133734480-83e0a7dc-aedf-482f-a0db-2a0b8f6df363.png" width = "720" height = "210">

在支持帧中寻找最相似RoI特征的过程
  
<img src="https://user-images.githubusercontent.com/43487243/133734491-1f1ee625-7aa3-4d4f-acfe-b8abcebd8deb.png" width = "570" height = "270">

多头注意力模块以进行时序聚合

<div align="left">
  
## Visualization Result：

<div align="center">

<img src="https://user-images.githubusercontent.com/43487243/133739298-b5ff639f-f17c-4e12-8f4f-89e97c21252a.png" width = "1024" height = "560">

四个序列上的可视化结果：每一行代表一个序列，绿色虚线框是当前帧的RoI特征；绿色实线框集后续帧的红点代表当前帧的点对应于支持帧最相似点的位置；图片下标的数字代表每一帧时序注意力的权重

<div align="left">

# TAM: Temporal Adaptive Module for Video Recognition(ICCV2021)
## Code Link： https://github.com/liu-zhy/temporal-adaptive-module
## Motivation: 解决视频动作识别任务中无法充分挖掘时序信息的问题
## Method: 首先分析了现有方法挖掘时序信息的不足，如：3D卷积和时空注意力模型都是视频无关的固定运算（在视频之间共享权重），无法自适应地调整，这种video-invariant的建模方式可能缺乏足够的表达能力（缺乏处理视频变化的灵活性）。所以为了有效地捕捉视频中的复杂运动模式，作者提出了一种全新的时序自适应模块TAM ，以自适应的方式学习视频中的时空线索。TAM主要由局部分支和全局分支组成，将时序建模核的参数分解成时序敏感的重要性权重和时序无关的时序聚合卷积。作者认为这样的分解方式，可以互补地学习视频中运动模式的多样性。

<div align="center">

<img src="https://user-images.githubusercontent.com/43487243/133741138-d2dcd22f-239a-46b7-b4c4-d448059fe1be.png" width = "1024" height = "454">

动机图：TAM与3D卷积以及时序注意力机制的对比

![TAM](https://user-images.githubusercontent.com/43487243/133741466-f0613403-cc48-4130-beb4-dbd318a7f3e8.png)

TAM结构图：TAM将时序自适应核的学习过程分解为局部分支和全局分支；全局分支基于全局时序信息生成视频自适应的动态卷积核以聚合时序信息，这种方式对时序位置不敏感，忽略了局部间的差异性；
而局部分支使用带有局部时序视野的1D卷积学习视频的局部结构信息，生成对时序位置敏感的重要性权重，以弥补全局分支存在的不足。

<img src="https://user-images.githubusercontent.com/43487243/133742089-3cede6ad-5bb5-443a-baad-cadeb4f9af9b.png" width = "630" height = "200">

TAM与传统3D卷积方法对比：回顾一下普通的时间卷积如何聚合时空视觉信息，W是卷积核的权重，与推理中的输入视频样本无关，这种方式忽略了视频的动态变化；因此提出了一种视频自适应聚合：
其中g可以看作是一个内核生成器函数，由G生成的核可以进行自适应卷积，但具有跨时间维度的共享性和时序位置不变性；为了解决这个问题，TAM提出了局部分支，生成时序位置敏感的重要性图。

